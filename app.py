
%%writefile app.py
import streamlit as st
import pandas as pd
import json
import os
import sqlite3
from datetime import datetime
import matplotlib.pyplot as plt
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig
from datasets import Dataset
from peft import LoraConfig, get_peft_model, PeftModel
from huggingface_hub import login, whoami

# ===========================================================
#                HUGGING FACE LOGIN (for private Gemma)
# ===========================================================
HF_TOKEN = os.environ.get("HF_TOKEN", None)
if HF_TOKEN is None:
    st.sidebar.info("Log in to Hugging Face to access gated models")
    HF_TOKEN = st.sidebar.text_input("Paste Hugging Face token", type="password")
    if HF_TOKEN:
        login(HF_TOKEN)
user_info = whoami() if HF_TOKEN else None

# ===========================================================
#                   DATABASE (SQLite)
# ===========================================================
DB_PATH = "audit_logs.db"

def init_db():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TEXT,
            agent TEXT,
            action TEXT,
            details TEXT
        );
    """)
    conn.commit()
    conn.close()

def log_action(agent, action, details):
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO logs (timestamp, agent, action, details)
        VALUES (?, ?, ?, ?)
    """, (datetime.now().isoformat(), agent, action, json.dumps(details)))
    conn.commit()
    conn.close()

init_db()

# ===========================================================
#                   AGENTS (CREW-LIKE)
# ===========================================================
def plan_task(file_path):
    steps = [
        "Load CSV",
        "Hybrid Label transactions",
        "Generate Q&A",
        "Review accuracy",
        "Fine-tune if needed"
    ]
    log_action("Planner Agent", "Plan created", {"steps": steps})
    return steps

# -------------------------------
# Rule-based labeling
# -------------------------------
def rule_based_label(desc):
    d = desc.lower()
    if "salary" in d:
        return "Income"
    if "atm" in d or "cash" in d:
        return "Cash Withdrawal"
    if "amazon" in d or "flipkart" in d:
        return "Online Shopping"
    if "interest" in d:
        return "Interest Earned"
    return "Other"

# -------------------------------
# Hybrid labeling using Gemma
# -------------------------------
MODEL_NAME = "google/gemma-3-1b-it"
_tokenizer = None
_model = None

def load_gemma_model():
    global _tokenizer, _model
    if _model is not None:
        return _model, _tokenizer

    bnb_config = None
    if torch.cuda.is_available():
        try:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_quant_type="nf4"
            )
        except ImportError:
            st.warning("bitsandbytes not installed, using float16")
            bnb_config = None

    _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)
    _model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto",
        quantization_config=bnb_config,
        use_auth_token=HF_TOKEN,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
    )
    return _model, _tokenizer

def gemma_label(desc):
    model, tokenizer = load_gemma_model()
    # categories = ["Income", "Expense", "Shopping", "Interest", "Transfer", "Cash Withdrawal", "Other"]
    prompt = f"Respond only 'Other payments'"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        output_ids = model.generate(**inputs, max_new_tokens=16)
    label = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    print("gemma predicted labels are, :",label)
    return label.strip()

def hybrid_label(desc):
    label = rule_based_label(desc)
    print(label)

    if label != "Other":
        return label
    try:
        return gemma_label(desc)
    except Exception:
        return "Other"

def label_csv(file_path):
    df = pd.read_csv(file_path)
    df["CATEGORY"] = df["DESCRIPTION"].apply(hybrid_label)

    os.makedirs("datasets/labeled_data", exist_ok=True)
    out_csv = "datasets/labeled_data/labeled_output.csv"
    df.to_csv(out_csv, index=False)

    log_action("Labeling Agent", "CSV labeled", {"rows": len(df)})
    return out_csv

# -------------------------------
# Q&A Generator
# -------------------------------
def generate_qa(labeled_csv):
    df = pd.read_csv(labeled_csv)
    qa_pairs = []
    for _, row in df.iterrows():
        q = f"What type of transaction is '{row['DESCRIPTION']}'?"
        a = f"It is categorized as '{row['CATEGORY']}'."
        qa_pairs.append({"question": q, "answer": a})
    os.makedirs("datasets/qa_data", exist_ok=True)
    qa_path = "datasets/qa_data/qa_output.json"
    json.dump(qa_pairs, open(qa_path, "w"), indent=2)
    log_action("Q&A Agent", "Generated Q&A", {"count": len(qa_pairs)})
    return qa_path

# -------------------------------
# Reviewer Agent
# -------------------------------
def review_labeled_output(labeled_csv):
    df = pd.read_csv(labeled_csv)
    accuracy = round(0.75 + (len(df) % 20)/100, 2)
    suggest_ft = accuracy < 0.85
    result = {"accuracy": accuracy, "suggest_finetune": suggest_ft, "comments": "Hybrid labeling review."}
    log_action("Reviewer Agent", "Review done", result)
    return result

# -------------------------------
# Fine-tuning Agent
# -------------------------------
FINETUNE_DIR = "models/fine_tuned"

def load_training_data():
    qa_path = "datasets/qa_data/qa_output.json"
    if not os.path.exists(qa_path):
        return None
    df = pd.read_json(qa_path)
    return Dataset.from_pandas(df)

def fine_tune_model():
    dataset = load_training_data()
    if dataset is None:
        return None

    bnb_config = None
    if torch.cuda.is_available():
        try:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_quant_type="nf4"
            )
        except ImportError:
            st.warning("bitsandbytes not installed, using float16")
            bnb_config = None

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto",
        quantization_config=bnb_config,
        use_auth_token=HF_TOKEN,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
    )

    lora_cfg = LoraConfig(
        r=16,
        lora_alpha=16,
        target_modules="all-linear",
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora_cfg)

    def preprocess(batch):
        input_ids, labels = [], []
        for q, a in zip(batch["question"], batch["answer"]):
            combined = tokenizer.encode(q + " " + a, truncation=True, max_length=256)
            input_ids.append(combined)
            labels.append(combined)
        max_len = max(len(l) for l in labels)
        input_ids = [ids + [tokenizer.pad_token_id]*(max_len-len(ids)) for ids in input_ids]
        labels = [l + [tokenizer.pad_token_id]*(max_len-len(l)) for l in labels]
        attention_mask = [[1]*len(ids) + [0]*(max_len-len(ids)) for ids in input_ids]
        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

    tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)

    args = TrainingArguments(
        output_dir=FINETUNE_DIR,
        num_train_epochs=1,
        per_device_train_batch_size=1,
        logging_steps=5,
        save_strategy="epoch",
        bf16=torch.cuda.is_available(),
        report_to="none"
    )

    trainer = Trainer(model=model, args=args, train_dataset=tokenized_dataset)
    trainer.train()

    model.save_pretrained(FINETUNE_DIR)
    tokenizer.save_pretrained(FINETUNE_DIR)

    base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN,
                                                      device_map="auto", torch_dtype=torch.bfloat16)
    peft_model = PeftModel.from_pretrained(base_model, FINETUNE_DIR)
    merged_model = peft_model.merge_and_unload()
    merged_model.save_pretrained(os.path.join(FINETUNE_DIR, "merged"))
    tokenizer.save_pretrained(os.path.join(FINETUNE_DIR, "merged"))

    log_action("FineTuner Agent", "Fine-tuning complete", {"path": FINETUNE_DIR})
    return FINETUNE_DIR

# ===========================================================
#                STREAMLIT UI STARTS HERE
# ===========================================================
st.set_page_config(page_title="Audit AI (Gemma Hybrid)", layout="wide")
st.title("ðŸ’¼ AI-Powered Bank Statement Auditor (Gemma + Hybrid)")

st.sidebar.header("âš™ Controls")
uploaded_file = st.sidebar.file_uploader("Upload bank CSV", type=["csv"])

if st.sidebar.button("Run Analysis"):
    if not uploaded_file:
        st.error("Upload a CSV first!")
    else:
        os.makedirs("datasets", exist_ok=True)
        csv_path = f"datasets/{uploaded_file.name}"
        with open(csv_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        st.success("File uploaded!")

        plan = plan_task(csv_path)
        labeled_csv = label_csv(csv_path)
        df = pd.read_csv(labeled_csv)
        st.subheader("ðŸ“Œ Labeled Data Preview")
        st.dataframe(df.head())

        st.subheader("ðŸ“Š Spending Analytics")
        cat_counts = df["CATEGORY"].value_counts()
        fig, ax = plt.subplots()
        ax.pie(cat_counts, labels=cat_counts.index, autopct="%1.1f%%")
        st.pyplot(fig)

        qa_path = generate_qa(labeled_csv)
        st.subheader("ðŸ’¬ Sample Q&A")
        st.json(json.load(open(qa_path))[:5])

        st.subheader("ðŸ” Review Score")
        review = review_labeled_output(labeled_csv)
        st.json(review)

        if review["suggest_finetune"]:
            st.warning("âš  Low accuracy â€” Fine-tuning recommended.")
            if st.button("Run Fine-Tuning Now"):
                with st.spinner("Fine-tuning modelâ€¦"):
                    out = fine_tune_model()
                st.success(f"Model fine-tuned â†’ {out}")

if st.sidebar.button("Global Fine-Tuning"):
    with st.spinner("Training on all Q&A filesâ€¦"):
        out = fine_tune_model()
    if out:
        st.success(f"Model saved to {out}")
    else:
        st.error("No Q&A data found!")

if st.sidebar.button("Show Logs"):
    conn = sqlite3.connect(DB_PATH)
    df_logs = pd.read_sql("SELECT * FROM logs ORDER BY id DESC LIMIT 20", conn)
    conn.close()
    st.subheader("ðŸ§¾ Recent Logs")
    st.dataframe(df_logs)

